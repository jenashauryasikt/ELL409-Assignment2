{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural Networks\n",
    "\n",
    "In this notebook, we will implement a fully-connected feed-forward neural network on the MNIST dataset and regularize it with early stopping, norm penalty, dropout and batch normalization.\n",
    "\n",
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import Pickle\n",
    "\n",
    "# Additional Funcitons\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "# check device\n",
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 0.5\n",
    "N_EPOCHS = 50\n",
    "RANDOM_SEED = 32\n",
    "N_HIDDEN = 30\n",
    "\n",
    "N_CLASSES = 10\n",
    "N_INPUTS = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Multi-class classification using Neural Network\"\"\"\n",
    "class NN_iter(object):\n",
    "\n",
    "    def __init__(self, sizes, l1 = 0, l2 = 0, batch_size = 64, learning_rate = 0.001, \n",
    "                 epochs = 100, loss = \"cross\", esp = 10, dropout_p = 0, batch_n = False):\n",
    "        \"\"\"Sizes contains the number of neurons in each layer including\n",
    "        the input and output layer. \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.loss = loss\n",
    "        self.l1_coeff = l1\n",
    "        self.l2_coeff = l2\n",
    "        self.esp = esp\n",
    "        self.dropout_p = dropout_p\n",
    "        self.batch_n = batch_n\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        \"\"\"Output of the neural network\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            x = sigmoid(np.dot(w, x)+b)\n",
    "        return x\n",
    "\n",
    "    def fit(self, train_data):\n",
    "        \"\"\"Fit the weights and biases according to training data\"\"\"\n",
    "        n = len(train_data)\n",
    "        for j in range(self.epochs):\n",
    "            random.shuffle(train_data)\n",
    "            mini_batches = [train_data[k:k+ self.batch_size]\n",
    "                for k in range(0, n, self.batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.SGD(mini_batch)\n",
    "    \n",
    "    def fit_early_stop (self, train_data, valid_data):\n",
    "        n = len(train_data)\n",
    "        i = 0;\n",
    "        for j in range(self.epochs):\n",
    "            last_loss = self.error(self.predict(valid_data), train_data[:, -1])\n",
    "            random.shuffle(train_data)\n",
    "            mini_batches = [training_data[k:k+ self.batch_size]\n",
    "                for k in range(0, n, self.batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.SGD(mini_batch)\n",
    "            curr_loss = self.error(self.predict(valid_data), train_data[:, -1])\n",
    "            if (i > self.esp): break\n",
    "            if (curr_loss > last_loss): \n",
    "                i += 1\n",
    "            else:\n",
    "                i = 0\n",
    "            \n",
    "    def predict (self, test_data):\n",
    "        return [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        \n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Check Number of correct outputs in Test Data\"\"\"\n",
    "        test_results = self.predict(test_data)\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def SGD(self, mini_batch):\n",
    "        \"\"\"Perform Stochstic Gradient Descent\"\"\"\n",
    "        dropout_b = [np.random.binomial(1, 1-self.dropout_p, size = b.shape) for b in self.biases]\n",
    "        dropout_w = [np.random.binomial(1, 1-self.dropout_p, size = w.shape) for w in self.weights]\n",
    "        del_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        del_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            addn_del_b, addn_del_w = self.backprop(x, y)\n",
    "            del_b = [db+adb for db, adb in zip(del_b, addn_del_b)]\n",
    "            del_w = [dw+adw for dw, adw in zip(del_w, addn_del_w)]\n",
    "        self.weights = [(w-self.alpha*dw-self.apha*norm_pen_grad(w))*drop_w\n",
    "                        for w, dw, drop_w in zip(self.weights, del_w, dropout_w)]\n",
    "        self.biases = [(b-self.alpha*db)*drop_b\n",
    "                       for b, db, drop_b in zip(self.biases, del_b, dropout_b)]\n",
    "                         \n",
    "    def norm_pen_grad (self, weight):\n",
    "        return self.l1_coeff*np.sign(w) + self.l2_coeff*w\n",
    "                         \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return the gradient corresponding to one element using backpropogation\"\"\"\n",
    "        del_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        del_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = delta_loss (zs[-1], activations[-1], y)\n",
    "        del_b[-1] = delta\n",
    "        del_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            del_b[-l] = delta\n",
    "            del_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (del_b, del_w)\n",
    "    \n",
    "    def delta_loss (self, z, a, y):\n",
    "        \"\"\"Partial derivative wrt cost function\"\"\"\n",
    "        if (self.loss == \"mse\"):\n",
    "            return (a-y) * sigmoid_prime (z)\n",
    "        else: \n",
    "            return (a-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot (y):\n",
    "    oh = np.zeros((N_CLASSES, 1))\n",
    "    oh[y] = 1.0\n",
    "    return oh\n",
    "\n",
    "def reshape_inputs_tiny (xtrain, ytrain, xtest, ytest):\n",
    "    inp_tr = [np.reshape(xi, (N_INPUTS, 1)) for xi in xtrain]\n",
    "    op_tr = [one_hot(yi) for yi in ytrain]\n",
    "    train_set = zip (inp_tr, op_tr)\n",
    "    inp_te = [np.reshape(xi, (N_INPUTS, 1)) for xi in xtest]\n",
    "    test_set = zip (inp_te, ytest)\n",
    "    return train_set, test_set\n",
    "\n",
    "def reshape_inputs_mnist (train, test):\n",
    "    inp_tr = [np.reshape(xi, (N_INPUTS, 1)) for xi in train[0]]\n",
    "    op_tr = [one_hot(yi) for yi in train[1]]\n",
    "    train_set = zip (inp_tr, op_tr)\n",
    "    inp_te = [np.reshape(xi, (N_INPUTS, 1)) for xi in test[0]]\n",
    "    test_set = zip (inp_te, test[1])\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import os\n",
    "\n",
    "# Getting the TinyImageNet Dataset\n",
    "here = os.path.dirname(os.path.realpath('__file__'))\n",
    "subdir = \"TinyImageNet\"\n",
    "\n",
    "# Training Set  features for Standard Neural Network (NN) \n",
    "filepath = os.path.join(here, subdir, \"train_x.npy\")\n",
    "xtrain_cnn = np.load(filepath)\n",
    "xtrain_bnn = xtrain_cnn.reshape(xtrain_cnn.shape[0], -1).tolist()\n",
    "\n",
    "# Training Set labels \n",
    "filepath = os.path.join(here, subdir, \"train_y.npy\")\n",
    "ytrain = np.load(filepath).tolist\n",
    "\n",
    "# Test Set features for Standard Neural Network (NN)\n",
    "filepath = os.path.join(here, subdir, \"test_x.npy\")\n",
    "xtest_cnn = np.load(filepath)\n",
    "xtest_bnn = xtest_cnn.reshape(xtest_cnn.shape[0], -1).tolist()\n",
    "\n",
    "# Test Set labels \n",
    "filepath = os.path.join(here, subdir, \"test_y.npy\")\n",
    "ytest = np.load(filepath)\n",
    "print (ytest.shape)\n",
    "ytest = ytest.tolist()\n",
    "\n",
    "# Training Set and Test Set for NN\n",
    "train_bnn, test_bnn = reshape_inputs_tiny(xtrain_bnn, ytrain, xtest_bnn, ytest)\n",
    "#random.shuffle(train_bnn)\n",
    "#random.shuffle(test_bnn)\n",
    "\n",
    "#print (len(train_bnn))\n",
    "#print (xtest_bnn.size)\n",
    "#print (ytrain)\n",
    "#print (ytest.size)\n",
    "#file = open (filepath)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.open('../data/mnist.pkl.gz', 'rb')\n",
    "trd1, vd1, ted1 = Pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "train_set, test_set = reshape_inputs_mnist (trd1, ted1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_train(model, train_data):\n",
    "    '''\n",
    "    Function for computing the accuracy of the predictions over the training set\n",
    "    '''\n",
    "    n = len(train_data)\n",
    "    train_results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                        for (x, y) in train_data]\n",
    "    return float(sum(int(x == y) for (x, y) in train_results))/float(n)\n",
    "\n",
    "\n",
    "def get_accuracy_test (model, test_data)\n",
    "    n = len(test_data)\n",
    "    train_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in train_data]\n",
    "    return float(sum(int(x == y) for (x, y) in train_results))/float(n)\n",
    "\n",
    "\n",
    "def plot_losses(train_losses, valid_losses):\n",
    "    '''\n",
    "    Function for plotting training and test losses\n",
    "    '''\n",
    "    \n",
    "    # temporarily change the style of the plots to seaborn \n",
    "    plt.style.use('seaborn')\n",
    "\n",
    "    train_losses = np.array(train_losses) \n",
    "    valid_losses = np.array(valid_losses)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (8, 4.5))\n",
    "\n",
    "    ax.plot(train_losses, color='green', label='Training loss') \n",
    "    ax.plot(valid_losses, color='orange', label='Testing loss')\n",
    "    ax.set(title=\"Loss across epochs\", \n",
    "            xlabel='Epoch',\n",
    "            ylabel='Loss') \n",
    "    ax.legend()\n",
    "    fig.show()\n",
    "    \n",
    "    # change the plot style to default\n",
    "    plt.style.use('ggplot')\n",
    "\n",
    "def train (train_set, model):\n",
    "    '''\n",
    "    Function for the training one epoch\n",
    "    '''\n",
    "\n",
    "    model.fit(train_set)\n",
    "    running_loss = 0\n",
    "    \n",
    "    for X, y_true in train_set:  \n",
    "        y_hat = np.argmax(self.feedforward(X)) \n",
    "        running_loss += (y_hat-y_true)**2\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_set)\n",
    "    return model, epoch_loss\n",
    "\n",
    "def train_es (train_set, test_set, model):\n",
    "    '''\n",
    "    Function for the training one epoch\n",
    "    '''\n",
    "\n",
    "    model.fit_early_stop (train_set, test_set)\n",
    "    running_loss = 0\n",
    "    \n",
    "    for X, y_true in train_set:  \n",
    "        y_hat = np.argmax(self.feedforward(X)) \n",
    "        running_loss += (y_hat-y_true)**2\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_set)\n",
    "    return model, epoch_loss\n",
    "\n",
    "def test_loss (test_set, model):\n",
    "    '''\n",
    "    Function for the getting test loss for each epoch\n",
    "    '''\n",
    "    running_loss = 0\n",
    "    \n",
    "    for X, y_true in train_set:  \n",
    "        y_hat = np.argmax(self.feedforward(X)) \n",
    "        running_loss += (y_hat-y_true)**2\n",
    "        \n",
    "    epoch_loss = running_loss / len(test_set)\n",
    "    return model, epoch_loss\n",
    "\n",
    "def training_loop (model, train_set, test_set, epochs, print_every=1, es = False):\n",
    "    '''\n",
    "    Function defining the entire training loop\n",
    "    '''\n",
    "    \n",
    "    # set objects for storing metrics\n",
    "    best_loss = 1e10\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    " \n",
    "    # Train model\n",
    "    for epoch in range(0, epochs):\n",
    "\n",
    "        # training\n",
    "        model, train_loss = train (train_set, model) if (!es) else train_es (train_set, test_set, model)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # test\n",
    "        model, test_loss = test_loss(test_set, model)\n",
    "        test_losses.append(valid_loss)\n",
    "\n",
    "        if epoch % print_every == (print_every - 1):\n",
    "            \n",
    "            train_acc = get_accuracy_train(model, train_set)\n",
    "            valid_acc = get_accuracy_test(model, test_set)\n",
    "                \n",
    "            print(f'{datetime.now().time().replace(microsecond=0)}     '\n",
    "                  f'Epoch: {epoch}\\t'\n",
    "                  f'Train loss: {train_loss:.3f}\\t'\n",
    "                  f'Test loss: {valid_loss:.3f}\\t'\n",
    "                  f'Train accuracy: {100 * train_acc:.3f}\\t'\n",
    "                  f'Test accuracy: {100 * valid_acc:.3f}')\n",
    "            \n",
    "    plot_losses(train_losses, valid_losses)\n",
    "    \n",
    "    return model, (train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Neural Network\n",
    "\n",
    "### Unregularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_iter ([N_INPUTS, N_HIDDES, N_OUTPUTS], learning_rate = LEARNING_RATE, epochs = 1, batch_size = BATCH_SIZE)\n",
    "\n",
    "model = training_loop (model, train_set, test_set, N_EPOCHS, print_every = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_iter ([N_INPUTS, N_HIDDEN, N_OUTPUTS], learning_rate = LEARNING_RATE, epochs = 1, batch_size = BATCH_SIZE)\n",
    "\n",
    "model = training_loop (model, train_set, test_set, N_EPOCHS, print_every = 1, es = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norm Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_iter ([N_INPUTS, N_HIDDEN, N_OUTPUTS], learning_rate = LEARNING_RATE, epochs = 1, batch_size = BATCH_SIZE)\n",
    "\n",
    "model = training_loop (model, train_set, test_set, N_EPOCHS, print_every = 1, l1 = 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_iter ([N_INPUTS, N_HIDDEN, N_OUTPUTS], learning_rate = LEARNING_RATE, epochs = 1, batch_size = BATCH_SIZE)\n",
    "\n",
    "model = training_loop (model, train_set, test_set, N_EPOCHS, print_every = 1, l2 = 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_iter ([N_INPUTS, N_HIDDEN, N_OUTPUTS], learning_rate = LEARNING_RATE, epochs = 1, batch_size = BATCH_SIZE)\n",
    "\n",
    "model = training_loop (model, train_set, test_set, N_EPOCHS, print_every = 1, batch_n = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_iter ([N_INPUTS, N_HIDDEN, N_OUTPUTS], learning_rate = LEARNING_RATE, epochs = 1, batch_size = BATCH_SIZE)\n",
    "\n",
    "model = training_loop (model, train_set, test_set, N_EPOCHS, print_every = 1, drop_p = 0.015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Learning ++\n",
    "\n",
    "Here we use our neural network implemented for part 2 of the assignment as one of the two classifiers in 3.1 and the classifier in 3.4\n",
    "\n",
    "## 3.1 Training on Three MNIST classes\n",
    "\n",
    "Here we train out neural network on 3 classes of the MNIST dataset, taken in the ratio of 70:25:5 (3500, 1250 and 250 samples each) using Cross Entropy and Mean Squared Error Loss functions and then use L2 regularization to improve on accuracy metrics.\n",
    "\n",
    "### Importing 3 classes from MNIST\n",
    "\n",
    "We create a dataset having 70:20:5 split among 3 classes of the MNIST dataset and then do an 80:20 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_inputs_mnist_3class(tr_d, te_d):\n",
    "    tot_data = list()\n",
    "    trianing_data = list()\n",
    "    test_data = list()\n",
    "    c1 = 0\n",
    "    c2 = 0\n",
    "    c3 = 0\n",
    "    for x, y in zip(tr_d[0], tr_d[1]):\n",
    "        if (y == 1) and (c1 < 3500):\n",
    "            tot_data.append([np.reshape(x, (784, 1)), y])\n",
    "            c1 += 1\n",
    "        if (y == 2) and (c2 < 1250):\n",
    "            tot_data.append([np.reshape(x, (784, 1)), y])\n",
    "            c2 += 1\n",
    "        if (y == 3) and (c3 < 250):\n",
    "            tot_data.append([np.reshape(x, (784, 1)), y])\n",
    "            c3 += 1\n",
    "    random.shuffle (tot_data)\n",
    "    count = 0\n",
    "    for x, y in tot_data:\n",
    "        if (count <= 0.8*len(tot_data)):\n",
    "            training_data.append([x, one_hot(y)])\n",
    "        else:\n",
    "            test_data.append([x, y])\n",
    "        count += 1\n",
    "    return (training_data, test_data)\n",
    "\n",
    "f = gzip.open('../data/mnist.pkl.gz', 'rb')\n",
    "trd2, vd2, ted2 = Pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "train_set_mnist3, test_set_mnist3 = reshape_inputs_mnist_3class (trd2, ted2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unregularized Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_iter ([N_INPUTS, N_HIDDEN, N_OUTPUTS], learning_rate = LEARNING_RATE, epochs = 1, batch_size = BATCH_SIZE)\n",
    "\n",
    "model = training_loop (model, train_set_mnist3, test_set_mnist3, N_EPOCHS, print_every = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unregulaized MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_iter ([N_INPUTS, N_HIDDEN, N_OUTPUTS], learning_rate = LEARNING_RATE, epochs = 1, batch_size = BATCH_SIZE)\n",
    "\n",
    "model = training_loop (model, train_set_mnist3, test_set_mnist3, N_EPOCHS, print_every = 1, loss=\"mse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_iter ([N_INPUTS, N_HIDDEN, N_OUTPUTS], learning_rate = LEARNING_RATE, epochs = 1, batch_size = BATCH_SIZE)\n",
    "\n",
    "model = training_loop (model, train_set_mnist3, test_set_mnist3, N_EPOCHS, print_every = 1, l2=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_iter ([N_INPUTS, N_HIDDEN, N_OUTPUTS], learning_rate = LEARNING_RATE, epochs = 1, batch_size = BATCH_SIZE)\n",
    "\n",
    "model = training_loop (model, train_set_mnist3, test_set_mnist3, N_EPOCHS, print_every = 1, loss=\"mse\", l2=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Cross Training on MNIST and SVHN\n",
    "\n",
    "Here we will train our neural network on MNIST and test on SVHN and vice-versa\n",
    "\n",
    "### Importing SVHN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_inputs_svhn (tr_d, te_d):\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, test_data)\n",
    "\n",
    "train_data_raw = io.loadmat('../data/SVHN/train_32x32.mat')\n",
    "test_data_raw = io.loadmat('../data/SVHN/test_32x32.mat')\n",
    "tr_X = train_data_raw['X']\n",
    "tr_y = train_data_raw['y']\n",
    "tr_y.flatten()\n",
    "tr_y[tr_y==10] = 0\n",
    "te_X = test_data_raw['X']\n",
    "te_y = test_data_raw['y']\n",
    "te_y.flatten()\n",
    "te_y[tr_y==10] = 0\n",
    "trd3 = [tr_X.astype(float32), tr_y]\n",
    "ted3 = [te_X.astype(float32), te_y]\n",
    "\n",
    "train_set_svhn, test_set_svhn = reshape_inputs_svhn (trd3, ted3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Training on 5 classes\n",
    "\n",
    "Here we use 5 classes of the MNIST dataset to train the neural network and tested it on the test set. \n",
    "\n",
    "### Importing Dataset\n",
    "\n",
    "We create a dataset such that the training data has 5 classes and the test data has the other five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_inputs_mnist_5class():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_data = list()\n",
    "    test_data = list()\n",
    "    for x, y in zip(tr_d[0], tr_d[1]):\n",
    "        if (y < 5):\n",
    "            training_data.append([np.reshape(x, (784, 1)), one_hot(y)])\n",
    "    for x, y in zip(te_d[0], te_d[1]):\n",
    "        if (y >= 5):\n",
    "            test_data.append([np.reshape(x, (784, 1)), y])\n",
    "    return (training_data, test_data)\n",
    "\n",
    "f = gzip.open('../data/mnist.pkl.gz', 'rb')\n",
    "trd4, vd4, ted4 = Pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "train_set_mnist5, test_set_mnist5 = reshape_inputs_mnist_3class (trd4, ted4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_iter ([N_INPUTS, N_HIDDEN, N_OUTPUTS], learning_rate = LEARNING_RATE, epochs = 1, batch_size = BATCH_SIZE)\n",
    "\n",
    "model = training_loop (model, train_set_mnist5, test_set_mnist5, N_EPOCHS, print_every = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
